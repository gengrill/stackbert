{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StackSym.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7k68JmRVYXF"
      },
      "source": [
        "!pip install fairseq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku1pTLnvWQ11"
      },
      "source": [
        "# Apex can be installed to make pretraining and finetuning faster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mavZQPXCVaJ5"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n",
        "  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n",
        "  --global-option=\"--fast_multihead_attn\" ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEol2-qTVcAn"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npTGe4yJVcVs"
      },
      "source": [
        "# Pretrain model!\n",
        "# Please make sure to change paths as appropriate\n",
        "\n",
        "!fairseq-train drive/MyDrive/varRecovery-New/data-bin/pretrain-32bit-ideal \\\n",
        "    --task masked_lm \\\n",
        "    --criterion masked_lm \\\n",
        "    --arch roberta_base \\\n",
        "    --sample-break-mode none \\\n",
        "    --tokens-per-sample 512 \\\n",
        "    --optimizer adam \\\n",
        "    --adam-betas '(0.9,0.98)' --adam-eps 1e-6 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr-scheduler polynomial_decay \\\n",
        "    --max-tokens 512 \\\n",
        "    --lr 0.0001 \\\n",
        "    --fp16 \\\n",
        "    --warmup-updates 10000 \\\n",
        "    --total-num-update 305000 \\\n",
        "    --dropout 0.1 \\\n",
        "    --attention-dropout 0.1 \\\n",
        "    --weight-decay 0.01 \\\n",
        "    --update-freq 32 \\\n",
        "    --max-update 20000 \\\n",
        "    --log-format json \\\n",
        "    --log-interval 10 \\\n",
        "    --no-epoch-checkpoints \\\n",
        "    --save-dir drive/MyDrive/varRecovery-New/test_check/pretrain-32bit-ideal/ \\\n",
        "    --mask-prob 0.2 --random-token-prob 0.0 --leave-unmasked-prob 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMK3TizYVebk"
      },
      "source": [
        "# Finetune Model\n",
        "# Please make sure to change paths as appropriate\n",
        "\n",
        "!fairseq-train drive/MyDrive/varRecovery-New/data-bin/finetune-32bitideal-O3/ \\\n",
        "    --max-positions 512 \\\n",
        "    --batch-size 8 \\\n",
        "    --task sentence_prediction \\\n",
        "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "    --required-batch-size-multiple 1 \\\n",
        "    --arch roberta_base \\\n",
        "    --num-classes 9 \\\n",
        "    --criterion sentence_prediction \\\n",
        "    --classification-head-name sentence_prediction \\\n",
        "    --dropout 0.1 --attention-dropout 0.1 \\\n",
        "    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr-scheduler polynomial_decay --lr 1e-05 --max-epoch 15 --warmup-updates 500 \\\n",
        "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
        "    --find-unused-parameters \\\n",
        "    --no-epoch-checkpoints --update-freq 4 --log-format=json --log-interval 10 \\\n",
        "    --save-dir SavedModels/ \\\n",
        "    --restore-file drive/MyDrive/varRecovery-New/test_check/pretrain-32bit-ideal/checkpoint_best.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh1mUJoTVq3E"
      },
      "source": [
        "# Load finetuned model for inference\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "roberta = RobertaModel.from_pretrained('drive/MyDrive/varRecovery-New/test_check/finetune-32bitideal-O0', 'checkpoint_best.pt', data_name_or_path='drive/MyDrive/varRecovery-New/data-bin/finetune-32bitideal-O0', bpe=None)\n",
        "roberta.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFX6IoHDVvz8"
      },
      "source": [
        "# Load evaluation data\n",
        "import json\n",
        "data = json.load(open('drive/MyDrive/varRecovery-New/data-src/SPEC/SPEC.json'))\n",
        "# JSON is structured as per output of dataResolver\n",
        "# {\n",
        "#   \"func_name\": \"X\" : [<func disas>],\n",
        "#                \"Y\" : <size>,    \n",
        "# }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh7eDDo2V0Ws"
      },
      "source": [
        "labelFn = lambda label: roberta.task.label_dictionary.string(\n",
        "  [label + roberta.task.label_dictionary.nspecial]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StIMXf07WGE5"
      },
      "source": [
        "for func in data:\n",
        "  if \"O0\" not in func:\n",
        "    continue\n",
        "  tokens = data[func][\"X\"]\n",
        "  actualValue = data[func][\"Y\"]\n",
        "  if actualValue > 2048:\n",
        "    continue\n",
        "  encoded_tokens = roberta.task.source_dictionary.encode_line(tokens)\n",
        "  prediction = roberta.predict('sentence_prediction', encoded_tokens).argmax().item()\n",
        "  print(labelFn(prediction), actualValue)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}