{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformer import Transformer, TOK_LEN, PAD_TOK, MAX_INST_LEN\n",
    "from stacksyms import getFunctions, parseELF, parseDirectory\n",
    "\n",
    "functions = []\n",
    "\n",
    "#debugFilepath = 'data/cross-compile-dataset/bin/static/gcc/og/parallel'\n",
    "#debugFiledir = 'data/cross-compile-dataset/bin/static/gcc/og'\n",
    "debugFilepath = 'data/cross-compile-dataset/bin/static/gcc/og/sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2021 23:21:28 INFO:parseELF:Trying to parse data/cross-compile-dataset/bin/static/gcc/og/sum as ELF\n",
      "01/29/2021 23:21:28 INFO:parseDWARF:File has debug info..\n",
      "01/29/2021 23:21:29 INFO:parseELF:Found 87 functions.\n"
     ]
    }
   ],
   "source": [
    "#file2funcs = parseDirectory(debugFiledir)\n",
    "\n",
    "functions = parseELF(debugFilepath)\n",
    "#generateFeatures(functions)\n",
    "#assert(not any(filter(lambda x : PAD_TOK in x, data)))\n",
    "\n",
    "#main = list(filter(lambda x : x.name=='main', functions))[0]\n",
    "#main_x = generateFeatures(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12557\n"
     ]
    }
   ],
   "source": [
    "#print(sum(map(len,file2funcs.values())))\n",
    "#print([max(map(len,fun.x)) for fun in functions]) # no of insts\n",
    "#print(([sum(map(len,fun.x)) for fun in functions])) # no of toks\n",
    "#X = [[tok + [PAD_TOK] * (16-len(tok)) for tok in func] for func in data] # quite the blow-up.. not good\n",
    "#print(max(map(len,X[0])))\n",
    "#print(functions[0].x[:10])\n",
    "#print([[tok + [PAD_TOK] * (MAX_INST_LEN//TOK_LEN-len(tok)) for tok in func.x] for func in functions][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 10000\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_size = 256\n",
    "src_vocab_size = 2**16 # 2-byte sequence domain\n",
    "trg_vocab_size = src_vocab_size\n",
    "src_pad_idx = PAD_TOK # TODO: is this correct?\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "forward_expansion = 4\n",
    "dropout = 0.10\n",
    "max_len = MAX_INST_LEN\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', torch.cuda.get_device_name(0))\n",
    "\n",
    "X = [[tok + [PAD_TOK] * (MAX_INST_LEN-len(inst)) for tok in func] for func in data]\n",
    "x = torch.tensor(X) #.to(device)\n",
    "\n",
    "loader = iter(DataLoader(ELFDataSet(x, y_train), batch_size=32, shuffle=True))\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOK)\n",
    "\n",
    "# Tensorboard plots\n",
    "writer = SummaryWriter(\"runs/loss_plot\")\n",
    "step   = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "    model.eval()\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        inp_data = batch.src.to(device) # TODO fix\n",
    "        target = batch.trg.to(device)   # TODO fix\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "        # reshape and remove the start token\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        # clip to avoid exploding gradient issues\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step() # stochastic gradient descent\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dwarf_import.model.module.Module'>\n",
      "<class 'dwarf_import.model.module.Module'>\n"
     ]
    }
   ],
   "source": [
    "from elftools.elf.elffile import ELFFile\n",
    "from dwarf_import.model.module import Module\n",
    "from dwarf_import.io.dwarf_import import DWARFDB, DWARFImporter, place_component_in_module_tree\n",
    "\n",
    "elf = ELFFile(open(debugFilepath, 'rb'))\n",
    "module = Module()\n",
    "dwarfDB = DWARFDB(elf)\n",
    "importer = DWARFImporter(dwarfDB, dict())\n",
    "for component in importer.import_components():\n",
    "    place_component_in_module_tree(module, component)\n",
    "\n",
    "for cu in module.children():\n",
    "    if type(cu) == Module:\n",
    "        pass\n",
    "    elif type(cu) == Component:\n",
    "        pass\n",
    "    print(type(cu))\n",
    "#print(elf.get_section_by_name('.text').data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stacksyms import parseDWARF\n",
    "module, importer = parseDWARF(debugFilepath)\n",
    "for cu in module.children():\n",
    "    for component in cu.children():\n",
    "        for func in component.functions:\n",
    "            print(func)\n",
    "    \n",
    "dwarfDB = importer._dwarf_db\n",
    "dwarfData = dwarfDB._pri\n",
    "print(dir(dwarfData))\n",
    "print(dwarfData._die_map)\n",
    "# TODO: we could get basic block info from DWARF.. maybe\n",
    "#       look at \"objdump --dwarf=line execFile\" and DWARF Spec Section 6.2\n",
    "#       \n",
    "#lineProg = dwarfData.get_line_program()\n",
    "#dir(lineProg)\n",
    "#types = firstUnit.types\n",
    "#globals = firstUnit.variables\n",
    "functions = firstUnit.functions\n",
    "print([func.name for func in functions])\n",
    "#scopeQueries = ['info scope ' + func for func in functions]\n",
    "disasQueries = ['disas /r ' + func.name for func in functions]\n",
    "gdbOut = staticGDB(debugFilepath, functions, disasQueries)\n",
    "#scopeResults, disasResults = results[0:len(results)//2], results[len(results)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: newCollectLocals(gdbOut, scopeQueries, functions) and newCollectDisas(gdbOut, disasQueries, functions)\n",
    "#for scope, func in zip(scopeResults, functions):\n",
    "#    collectLocals(scope, functions[func])\n",
    "for disas, func in zip(gdbOut, functions):\n",
    "    func.disas = [tuple(line.strip().split('\\\\t')) for line in disas[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, Y, Z = [], [], []\n",
    "for func in functions:\n",
    "    print('////////////////////////')\n",
    "    print(func.name, hex(func.start), func.frame_base)\n",
    "    for lvar in func.variables:\n",
    "        print(lvar.name, lvar.type, \"(bytesize = %d)\"%lvar.type.byte_size)\n",
    "        for loc in lvar.locations:\n",
    "            print(hex(loc.begin) + \" to \" + hex(loc.end) + \": \" \\\n",
    "            + str(loc.type)[13:]\n",
    "            + str(loc.expr))\n",
    "        print('')\n",
    "#    X += [func] #generateFeature(func, functions)]\n",
    "#    Y += [generateLabel(func, functions)]\n",
    "#    Z += [generateDebugLabel(func, functions)]\n",
    "#print([x+\" => \"+ str(y) for x,y in zip(X,Z)])\n",
    "\n",
    "#func = 'quotearg_n_style_colon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in functions:\n",
    "    print('////////////////////////')\n",
    "    print(func.name, func.frame_base)\n",
    "    for lvar in func.variables:\n",
    "        print(lvar.name, lvar.type, \"(bytesize = %d)\"%lvar.type.byte_size)\n",
    "        for loc in lvar.locations:\n",
    "    #        print(loc)\n",
    "            print(hex(loc.begin) + \" to \" + hex(loc.end) + \": \" \\\n",
    "    #            + str(loc.type)[13:]\n",
    "                + str(loc.expr))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateLabel(func, functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu = ELFFile(open(debugFilepath, mode='rb')).get_dwarf_info().iter_CUs().__next__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "generateDebugLabels.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
